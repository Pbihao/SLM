{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/llm_seg/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 15:31:27,770] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from models.slm import ScalableLM\n",
    "from models.config import LlamaCLConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "dataset = load_dataset(\"Stevross/mmlu\", \"high_school_european_history\", split=\"auxiliary_train\")\n",
    "# model = ScalableLM(LlamaCLConfig())\n",
    "# model.load_state_dict(torch.load(\"/home/user/bhpeng/SLM-llama/outputs/history/history/checkpoint-780/pytorch_model_retriever.bin\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PROMPT_TEMPLATE = {\n",
    "'with_sys': \"\"\"[INST] <<SYS>>\n",
    "{instruction}\n",
    "<</SYS>>\n",
    "\n",
    "{input} [/INST] \\n\"\"\",\n",
    "\n",
    "'without_sys': \"\"\"[INST] {input} [/INST] \\n\"\"\"\n",
    "}\n",
    "def extract_from_history(example):\n",
    "    question = example['question']\n",
    "    choices = example['choices']\n",
    "    output = example['answer']\n",
    "    input=\"Question:{}\\n Choices:{}\\n\".format(question, str(choices))\n",
    "    output = chr(ord('@')+int(output)+1)\n",
    "    return dict(\n",
    "        input=input,\n",
    "        output=output\n",
    "    )\n",
    "\n",
    "def prepare_data(example):\n",
    "    example = extract_from_history(example)\n",
    "    raw_text = example['instruction'] + \" ### \" + example['input'] if 'instruction' in example.keys() else \\\n",
    "                   example['input']\n",
    "    source = PROMPT_TEMPLATE['with_sys'].format_map(example) if 'instruction' in example.keys() else\\\n",
    "                 PROMPT_TEMPLATE['without_sys'].format_map(example)\n",
    "    source = tokenizer(\n",
    "            source,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    task = 'history'\n",
    "    return dict(\n",
    "        raw_text=raw_text,\n",
    "        input_ids=source['input_ids'],\n",
    "        attention_mask=source['attention_mask'],\n",
    "        task=task\n",
    "    ), example['output'] + \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A</s>\n",
      "Question:Dean, charged with murder, was present with her attorney at a preliminary examination when White, who was the defendant in a separate prosecution for concealing the body of the murder victim, testified for the prosecution against Dean. When called to testify at Dean's trial, White refused to testify, though ordered to do so. The prosecution offers evidence of White's testimony at the preliminary examination. The evidence is\n",
      " Choices:['admissible as former testimony.', 'admissible as past recollection recorded.', \"inadmissible, because it would violate White's privilege against selfincrimination. \", 'inadmissible, because it is hearsay, not within any exception']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "inps, outs = prepare_data(dataset[random.randint(0, 1000)])\n",
    "# out = model.generate(**inps)\n",
    "print(outs)\n",
    "# print(tokenizer.decode(out[0]))\n",
    "print(inps['raw_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
