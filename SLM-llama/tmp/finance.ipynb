{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from models.slm import ScalableLM\n",
    "from models.config import LlamaCLConfig\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "dataset = load_dataset(\"AdiOO7/llama-2-finance\", split='train')\n",
    "model = ScalableLM(LlamaCLConfig())\n",
    "model.load_state_dict(torch.load(\"/home/user/bhpeng/SLM-llama/outputs/finance/finance/checkpoint-222/pytorch_model_only_retriever.bin\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "PROMPT_TEMPLATE = {\n",
    "'with_sys': \"\"\"[INST] <<SYS>>\n",
    "{instruction}\n",
    "<</SYS>>\n",
    "\n",
    "{input} [/INST] \"\"\",\n",
    "\n",
    "'without_sys': \"\"\"[INST] {input} [/INST] \"\"\"\n",
    "}\n",
    "def extract_from_finance(example):\n",
    "    text = example['text']\n",
    "    m = re.search('### Instruction:(.+?)### Human:', text)\n",
    "    instruction = m.group(1).strip() if m else None\n",
    "    m = re.search('### Human:(.+?)### Assistant:', text)\n",
    "    input = m.group(1).strip() if m else None\n",
    "    output = text.split(\"### Assistant:\")[1].strip()\n",
    "    return dict(\n",
    "        instruction=instruction,\n",
    "        input=input,\n",
    "        output=output\n",
    "    )\n",
    "def prepare_data(example):\n",
    "    example = extract_from_finance(example)\n",
    "    raw_text = example['instruction'] + \" ### \" + example['input'] if 'instruction' in example.keys() else \\\n",
    "                   example['input']\n",
    "    source = PROMPT_TEMPLATE['with_sys'].format_map(example) if 'instruction' in example.keys() else\\\n",
    "                 PROMPT_TEMPLATE['without_sys'].format_map(example)\n",
    "    source = tokenizer(\n",
    "            source,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    task = 'finance'\n",
    "    return dict(\n",
    "        raw_text=raw_text,\n",
    "        input_ids=source['input_ids'],\n",
    "        attention_mask=source['attention_mask'],\n",
    "        task=task\n",
    "    ), example['output'] + \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "inps, outs = prepare_data(dataset[random.randint(0, 1000)])\n",
    "out = model.generate(**inps)\n",
    "print(outs)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
