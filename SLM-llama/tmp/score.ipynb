{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/alpaca/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-17 21:19:03,156] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "\n",
    "result_path = \"/data/bhpeng/SLM-llama/tmp/results/medical/result.json\"\n",
    "task = 'medical'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.9999999403953552, 1.0000001192092896], 'recall': [0.9999999403953552, 1.0000001192092896], 'f1': [0.9999999403953552, 1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.31.0)'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(label, predict):\n",
    "    # Calculate accuracy for multiple-choices questions and classification\n",
    "    # For generation model, we only select the first word to compare\n",
    "    label = re.sub(r'[^\\w\\s]', ' ', label)\n",
    "    predict = re.sub(r'[^\\w\\s]', ' ', predict)\n",
    "    assert len(label.split()) == 1, \"label can only contain one word\"\n",
    "    if len(predict.split()) == 0:\n",
    "        return 0\n",
    "    label = label.split()[0]\n",
    "    predict = predict.split()[0]\n",
    "    return int(label == predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8538064939690427 0.8456369483257489 0.8494767291804664\n"
     ]
    }
   ],
   "source": [
    "with open(result_path) as f:\n",
    "    results = json.load(f)\n",
    "if task in ['history', 'finance']:\n",
    "    score = 0\n",
    "    for result in tqdm(results):\n",
    "        label = result['label']\n",
    "        output = result['output']\n",
    "        score += accuracy_score(label, output)\n",
    "    score = score / len(results)\n",
    "    print(score)\n",
    "elif task in ['medical']:\n",
    "    bertscore = load(\"bertscore\")\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    for result in results:\n",
    "        labels.append(result['label'])\n",
    "        outputs.append(result['output'])\n",
    "    score = bertscore.compute(predictions=outputs, references=labels, lang='en')\n",
    "    for k in ['precision', 'recall', 'f1']:\n",
    "        score[\"avg_{}\".format(k)] = sum(score[k]) / len(results)\n",
    "    print(score['avg_precision'], score['avg_recall'], score['avg_f1'])\n",
    "else:\n",
    "    raise ValueError(\"No matched task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31111 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 37/31111 [00:41<9:43:36,  1.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m label \u001b[39m=\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m score \u001b[39m=\u001b[39m bertscore\u001b[39m.\u001b[39;49mcompute(predictions\u001b[39m=\u001b[39;49m[output], references\u001b[39m=\u001b[39;49m[label], lang\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     19\u001b[0m     scores[k] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m score[k]\n",
      "File \u001b[0;32m/data/miniconda3/envs/alpaca/lib/python3.10/site-packages/evaluate/module.py:432\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    435\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/miniconda3/envs/alpaca/lib/python3.10/site-packages/evaluate/module.py:481\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_feature_from_batch(batch)\n\u001b[0;32m--> 481\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_writer()\n\u001b[1;32m    482\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[39mfor\u001b[39;00m key, column \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/data/miniconda3/envs/alpaca/lib/python3.10/site-packages/evaluate/module.py:605\u001b[0m, in \u001b[0;36mEvaluationModule._init_writer\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[39m# Get cache file name and lock it\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilelock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     cache_file_name, filelock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_cache_file()  \u001b[39m# get ready\u001b[39;00m\n\u001b[1;32m    606\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m cache_file_name\n\u001b[1;32m    607\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilelock \u001b[39m=\u001b[39m filelock\n",
      "File \u001b[0;32m/data/miniconda3/envs/alpaca/lib/python3.10/site-packages/evaluate/module.py:270\u001b[0m, in \u001b[0;36mEvaluationModule._create_cache_file\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    268\u001b[0m filelock \u001b[39m=\u001b[39m FileLock(file_path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     filelock\u001b[39m.\u001b[39;49macquire(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    271\u001b[0m \u001b[39mexcept\u001b[39;00m Timeout:\n\u001b[1;32m    272\u001b[0m     \u001b[39m# If we have reached the max number of attempts or we are not allow to find a free name (distributed setup)\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[39m# We raise an error\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_process \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/data/miniconda3/envs/alpaca/lib/python3.10/site-packages/datasets/utils/filelock.py:282\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[0;34m(self, timeout, poll_intervall)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m             logger()\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    280\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLock \u001b[39m\u001b[39m{\u001b[39;00mlock_id\u001b[39m}\u001b[39;00m\u001b[39m not acquired on \u001b[39m\u001b[39m{\u001b[39;00mlock_filename\u001b[39m}\u001b[39;00m\u001b[39m, waiting \u001b[39m\u001b[39m{\u001b[39;00mpoll_intervall\u001b[39m}\u001b[39;00m\u001b[39m seconds ...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m             )\n\u001b[0;32m--> 282\u001b[0m             time\u001b[39m.\u001b[39;49msleep(poll_intervall)\n\u001b[1;32m    283\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[39m# Something did go wrong, so decrement the counter.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_thread_lock:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(result_path) as f:\n",
    "    results = json.load(f)\n",
    "if task in ['history', 'finance']:\n",
    "    score = 0\n",
    "    for result in tqdm(results):\n",
    "        label = result['label']\n",
    "        output = result['output']\n",
    "        score += accuracy_score(label, output)\n",
    "    score = score / len(results)\n",
    "    print(score)\n",
    "elif task in ['medical']:\n",
    "    scores = dict(precision=[], recall=[], f1=[], sum_precision=0., sum_recall=0., sum_f1=0.)\n",
    "    bertscore = load(\"bertscore\")\n",
    "    for result in tqdm(results):\n",
    "        label = result['label']\n",
    "        output = result['output']\n",
    "        score = bertscore.compute(predictions=[output], references=[label], lang='en')\n",
    "        for k in ['precision', 'recall', 'f1']:\n",
    "            scores[k] += score[k]\n",
    "            scores[\"sum_{}\".format(k)] += score[k][0]\n",
    "    for k in ['precision', 'recall', 'f1']:\n",
    "        scores['avg_{}'.format(k)] = scores['sum_{}'.format(k)] / len(results)    \n",
    "    print(result['avg_precision'], result['avg_recall'], result['avg_f1'])\n",
    "else:\n",
    "    raise ValueError(\"No matched task\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
